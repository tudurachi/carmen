    
from IPython import display
import math
from pprint import pprint
import pandas as pd
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='darkgrid', context='talk', palette='Dark2')
sns.set_context(context='notebook', font_scale=1.3, rc={'figure.figsize': (16,9)})

import praw

reddit = praw.Reddit(client_id='IHr2KoRnWZ3EhA', 
                     client_secret='AoGKF7utHV0WznFO9KzJdISxakI',
                     user_agent='Reddit WebScraping')

headlines = set()


for submission in reddit.subreddit('gretathunberg').new(limit=None):
    headlines.add(submission.title)
    display.clear_output()
    print(len(headlines))
    
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA

nltk.download('vader_lexicon')
sia = SIA()
results = []

for line in headlines:
    pol_score = sia.polarity_scores(line)
    pol_score['headline'] = line
    results.append(pol_score)

pprint(results[:10], width=100)

df = pd.DataFrame.from_records(results)
df.head()

df['label'] = 0
df.loc[df['compound'] > 0.2, 'label'] = 1
df.loc[df['compound'] < -0.2, 'label'] = -1
df.head()


df2 = df[['headline', 'label']]
df2.to_csv('gretathunberg.csv', mode='a', encoding='utf-8', index=False)

df3=df2.loc[df2['label'].isin(['1','-1'])]

print("Positive headlines:\n")
pprint(list(df3[df3['label'] == 1].headline)[:5], width=200)

print("\nNegative headlines:\n")
pprint(list(df3[df3['label'] == -1].headline)[:5], width=200)

print(df3.label.value_counts())
print(df3.label.value_counts(normalize=True) * 100)

fig, ax = plt.subplots(figsize=(8, 8))
counts = df3.label.value_counts(normalize=True) * 100
sns.barplot(x=counts.index, y=counts, ax=ax)
ax.set_xticklabels(['Negative', 'Positive'])
ax.set_ylabel("Percentage")
plt.show()

from nltk.tokenize import  RegexpTokenizer
nltk.download('punkt')

tokenizer = RegexpTokenizer(r'\w+')

from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = stopwords.words('english')
print(stop_words[:20])


def process_text(headlines):
    tokens = []
    for line in headlines:
        toks = tokenizer.tokenize(line)
        toks = [t.lower() for t in toks if t.lower() not in stop_words]
        tokens.extend(toks)
    
    return tokens

    
pos_lines = list(df3[df3.label == 1].headline)
pos_tokens = process_text(pos_lines)
pos_freq = nltk.FreqDist(pos_tokens)
pos_freq.most_common(10)
y_val = [x[1] for x in pos_freq.most_common()]


positive_distribution=pd.DataFrame(list(pos_freq.items()), columns = ["Word","Frequency"])

fig = plt.figure(figsize=(10,5))
plt.plot(y_val)
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Word Frequency Distribution (Positive)")
plt.show()

y_final = []
for i, k, z, t in zip(y_val[0::4], y_val[1::4], y_val[2::4], y_val[3::4]):
    y_final.append(math.log(i + k + z + t))

x_val = [math.log(i + 1) for i in range(len(y_final))]

fig = plt.figure(figsize=(10,5))
plt.xlabel("Words (Log)")
plt.ylabel("Frequency (Log)")
plt.title("Word Frequency Distribution (Positive)")
plt.plot(x_val, y_final)
plt.show()

neg_lines = list(df3[df3.label == -1].headline)
neg_tokens = process_text(neg_lines)
neg_freq = nltk.FreqDist(neg_tokens)
neg_freq.most_common(10)
n_val = [x[1] for x in neg_freq.most_common()]

negative_distribution=pd.DataFrame(list(neg_freq.items()), columns = ["Word","Frequency"])

fig = plt.figure(figsize=(10,5))
plt.plot(n_val)
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Word Frequency Distribution (Negative)")
plt.show()

n_final = []
for i, k, z in zip(y_val[0::3], y_val[1::3], y_val[2::3]):
    if i + k + z == 0:
        break
    n_final.append(math.log(i + k + z))

z_val = [math.log(i+1) for i in range(len(n_final))]

fig = plt.figure(figsize=(10,5))
plt.xlabel("Words (Log)")
plt.ylabel("Frequency (Log)")
plt.title("Word Frequency Distribution (Negative)")
plt.plot(z_val,n_final)
plt.show()

#############################################################

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer

X_train, X_test, y_train, y_test = train_test_split(df3['headline'], df3['label'], random_state = 0)
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

clf = MultinomialNB().fit(X_train_tfidf, y_train)

print(clf.predict(count_vect.transform(["Inspired by the Swedish teenage climate activist, the younger members of society are leading the fight against global"])))

print(clf.predict(count_vect.transform(["Greta Thunberg's trip home on a crowded German train"])))


#############################################################

import tkinter as tk


root= tk.Tk()

canvas1 = tk.Canvas(root, width = 500, height = 350)
canvas1.pack()


label1 = tk.Label(root, text='            Reddit:')
canvas1.create_window(100, 100, window=label1)

entry1 = tk.Entry (root)
canvas1.create_window(270, 100, window=entry1)


def values(): 
    global reddit
    reddit = float(entry1.get())  
    
    Prediction_result  = ('  Predicted Result: ', clf.predict(count_vect.transform(reddit)))
    label_Prediction = tk.Label(root, text= Prediction_result, bg='sky blue')
    canvas1.create_window(270, 280, window=label_Prediction)
    
button1 = tk.Button (root, text='      Predict      ',command=values, bg='green', fg='white', font=11)
canvas1.create_window(270, 220, window=button1)
 
root.mainloop()
